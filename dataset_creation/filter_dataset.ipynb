{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter datasets to ensure they are inline with eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify images without a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_dir = \"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data\"\n",
    "\n",
    "# image_files =  [file[:-4] for file in os.listdir(f\"{project_dir}/images\")]\n",
    "# label_YOLOv8 = [file[:-4] for file in os.listdir(f\"{project_dir}/labels_YOLOv8\")]\n",
    "# label_image =  [file[:-4] for file in os.listdir(f\"{project_dir}/labels_as_images\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-05_14.18.49--NAs--T1354-GFP_Burst_Frame_0_Granule_10\n",
      "Images       596833\n",
      "Label_YOLOv8 596833\n",
      "Label_image  595553\n"
     ]
    }
   ],
   "source": [
    "# print(image_files[0])\n",
    "# print(\"Images      \", len(image_files))\n",
    "# print(\"Label_YOLOv8\", len(label_YOLOv8))\n",
    "# print(\"Label_image \", len(label_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for im in image_files: # For every image\n",
    "#     if (im in label_YOLOv8) and (im in label_image): # If image has both yolo label and image label\n",
    "#         # Put valid image and labels into new folders\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/images/{im}.png\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/images/{im}.png\")\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/labels_as_images/{im}.png\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/labels_as_images/{im}.png\")\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/labels_YOLOv8/{im}.txt\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/labels_YOLOv8/{im}.txt\")\n",
    "#         count += 1\n",
    "# print(f\"Final dataset size is:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train, val, test datasets of given size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset sizes ---\n",
      "3500\n",
      "3500\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "compiled_datasets_name = \"compiled_datasets_16bit_large\"\n",
    "\n",
    "project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "project_output_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/{compiled_datasets_name}\"\n",
    "\n",
    "# Create folder structure\n",
    "Path(project_output_dir_FINAL).mkdir(parents=True, exist_ok=True)\n",
    "for s in ['train', 'val', 'test']:\n",
    "    Path(project_output_dir_FINAL+f\"/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "    for st in ['images', 'labels']:\n",
    "        Path(project_output_dir_FINAL+f\"/{s}/{st}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# image_files_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/images\")]\n",
    "img_directory = \"images_grayscale_16bit\" # \"images\"\n",
    "image_files_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/{img_directory}\")]\n",
    "# label_YOLOv8_FINAL = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_YOLOv8\")]\n",
    "label_image_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_as_images\")]\n",
    "\n",
    "TOTAL_SIZE = 10000\n",
    "subset_dataset = []\n",
    "for i in np.random.choice(image_files_FINAL, TOTAL_SIZE, replace=False):\n",
    "    subset_dataset.append(i)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(subset_dataset, subset_dataset, test_size=0.3, random_state=1)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "print(\"--- Dataset sizes ---\")\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "for im in x_train:\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/train/images/{im}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/train/labels/{im}.png\")\n",
    "\n",
    "for im in x_val: \n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/val/images/{im}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/val/labels/{im}.png\")\n",
    "\n",
    "for im in x_test: \n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/test/images/{im}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/test/labels/{im}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing dataset based on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 0\n",
      "Status: 1000\n",
      "Status: 2000\n",
      "Status: 3000\n",
      "Status: 4000\n",
      "Status: 5000\n",
      "Status: 6000\n",
      "Status: 7000\n",
      "Status: 8000\n",
      "Status: 9000\n",
      "Status: 10000\n",
      "Status: 11000\n",
      "Status: 12000\n",
      "Status: 13000\n",
      "Status: 14000\n",
      "Status: 15000\n",
      "Status: 16000\n",
      "Status: 17000\n",
      "Status: 18000\n",
      "Status: 19000\n",
      "Status: 20000\n",
      "Status: 21000\n",
      "Status: 22000\n",
      "Status: 23000\n",
      "Status: 24000\n",
      "Status: 25000\n",
      "Status: 26000\n",
      "Status: 27000\n",
      "Status: 28000\n",
      "Status: 29000\n",
      "Status: 30000\n",
      "Status: 31000\n",
      "Status: 32000\n",
      "Status: 33000\n",
      "Status: 34000\n",
      "Status: 35000\n",
      "Status: 36000\n",
      "Status: 37000\n",
      "Status: 38000\n",
      "Status: 39000\n",
      "Status: 40000\n",
      "Status: 41000\n",
      "Status: 42000\n",
      "Status: 43000\n",
      "Status: 44000\n",
      "Status: 45000\n",
      "Status: 46000\n",
      "Status: 47000\n",
      "Status: 48000\n",
      "Status: 49000\n",
      "Status: 50000\n",
      "Status: 51000\n",
      "Status: 52000\n",
      "Status: 53000\n",
      "Status: 54000\n",
      "Status: 55000\n",
      "Status: 56000\n",
      "Status: 57000\n",
      "Status: 58000\n",
      "Status: 59000\n",
      "Status: 60000\n",
      "Status: 61000\n",
      "Status: 62000\n",
      "Status: 63000\n",
      "Status: 64000\n",
      "Status: 65000\n",
      "Status: 66000\n",
      "Status: 67000\n",
      "Status: 68000\n",
      "Status: 69000\n",
      "Status: 70000\n",
      "Status: 71000\n",
      "Status: 72000\n",
      "Status: 73000\n",
      "Status: 74000\n",
      "Status: 75000\n",
      "Status: 76000\n",
      "Status: 77000\n",
      "Status: 78000\n",
      "Status: 79000\n",
      "Status: 80000\n",
      "Status: 81000\n",
      "Status: 82000\n",
      "Status: 83000\n",
      "Status: 84000\n",
      "Status: 85000\n",
      "Status: 86000\n",
      "Status: 87000\n",
      "Status: 88000\n",
      "Status: 89000\n",
      "Status: 90000\n",
      "Status: 91000\n",
      "Status: 92000\n",
      "Status: 93000\n",
      "Status: 94000\n",
      "Status: 95000\n",
      "Status: 96000\n",
      "Status: 97000\n",
      "Status: 98000\n",
      "Status: 99000\n",
      "Status: 100000\n",
      "Status: 101000\n",
      "Status: 102000\n",
      "Status: 103000\n",
      "Status: 104000\n",
      "Status: 105000\n",
      "Status: 106000\n",
      "Status: 107000\n",
      "Status: 108000\n",
      "Status: 109000\n",
      "Status: 110000\n",
      "Status: 111000\n",
      "Status: 112000\n",
      "Status: 113000\n",
      "Status: 114000\n",
      "Status: 115000\n",
      "Status: 116000\n",
      "Status: 117000\n",
      "Status: 118000\n",
      "Status: 119000\n",
      "Status: 120000\n",
      "Status: 121000\n",
      "Status: 122000\n",
      "Status: 123000\n",
      "Status: 124000\n",
      "Status: 125000\n",
      "Status: 126000\n",
      "Status: 127000\n",
      "Status: 128000\n",
      "Status: 129000\n",
      "Status: 130000\n",
      "Status: 131000\n",
      "Status: 132000\n",
      "Status: 133000\n",
      "Status: 134000\n",
      "Status: 135000\n",
      "Status: 136000\n",
      "Status: 137000\n",
      "Status: 138000\n",
      "Status: 139000\n",
      "Status: 140000\n",
      "Status: 141000\n",
      "Status: 142000\n",
      "Status: 143000\n",
      "Status: 144000\n",
      "Status: 145000\n",
      "Status: 146000\n",
      "Status: 147000\n",
      "Status: 148000\n",
      "Status: 149000\n"
     ]
    }
   ],
   "source": [
    "project_dir = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/DATASET_DIVIDED_ON_SIZE\"\n",
    "# project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "\n",
    "# # Create folder structure\n",
    "# for s in [10,20,30,40,50,60,70,80,90]:\n",
    "#     Path(project_dir+f\"/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "label_size_dataframe = {\n",
    "    'label_name': [],\n",
    "    'label_size': [],\n",
    "}\n",
    "# Only need labels\n",
    "label_image_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_as_images\")]\n",
    "for i in range(len(label_image_FINAL)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Status:\", i)\n",
    "    feature_image = cv2.imread(f\"{project_dir_FINAL}/labels_as_images/{label_image_FINAL[i]}.png\", cv2.IMREAD_GRAYSCALE)\n",
    "    label_size_dataframe['label_name'].append(label_image_FINAL[i])\n",
    "    label_size_dataframe['label_size'].append(feature_image[feature_image == 1].size / (1024*1024))\n",
    "\n",
    "pd.DataFrame(label_size_dataframe).to_csv(project_dir + \"/label_size_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\\DATASET_DIVIDED_ON_SIZE\\label_size_dataframe.csv\", index_col=0)\n",
    "# label_size_df = label_size_df[(0.1 < label_size_df['label_size']) & (label_size_df['label_size'] < 0.48)]\n",
    "# pd.DataFrame(label_size_df).to_csv(project_dir + \"/label_size_dataframe_with_deletions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data based on label size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3360, 3)\n",
      "      old_index                                         label_name  label_size\n",
      "0         66362  2020-02-05_14.36.40--NAs--T1354-GFP_Burst_Fram...    0.178321\n",
      "1         62902  2020-02-05_14.36.40--NAs--T1354-GFP_Burst_Fram...    0.185653\n",
      "2        104739  2020-02-05_14.38.46--NAs--T1354-GFP_Burst_Fram...    0.188028\n",
      "3         31524  2020-02-05_14.35.36--NAs--T1354-GFP_Burst_Fram...    0.184062\n",
      "4         45306  2020-02-05_14.35.36--NAs--T1354-GFP_Burst_Fram...    0.176215\n",
      "...         ...                                                ...         ...\n",
      "3355      86454  2020-02-05_14.37.45--NAs--T1354-GFP_Burst_Fram...    0.442201\n",
      "3356      70655  2020-02-05_14.37.45--NAs--T1354-GFP_Burst_Fram...    0.429742\n",
      "3357      74065  2020-02-05_14.37.45--NAs--T1354-GFP_Burst_Fram...    0.432381\n",
      "3358      89427  2020-02-05_14.37.45--NAs--T1354-GFP_Burst_Fram...    0.441193\n",
      "3359      47955  2020-02-05_14.35.36--NAs--T1354-GFP_Burst_Fram...    0.430477\n",
      "\n",
      "[3360 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "compiled_datasets_name = \"compiled_datasets_16bit_medium_equal_label_size\"\n",
    "\n",
    "project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "project_output_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/{compiled_datasets_name}\"\n",
    "\n",
    "# Create folder structure\n",
    "Path(project_output_dir_FINAL).mkdir(parents=True, exist_ok=True)\n",
    "for title in ['normal', 'gradient']:\n",
    "    Path(project_output_dir_FINAL+f\"/{title}\").mkdir(parents=True, exist_ok=True) # Dataset folder\n",
    "\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        Path(project_output_dir_FINAL+f\"/{title}/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "        for st in ['images', 'labels']:\n",
    "            Path(project_output_dir_FINAL+f\"/{title}/{s}/{st}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/EVEN_DATASET.csv\", index_col=0) \n",
    "# label_size_df = label_size_df['label_name']\n",
    "print(label_size_df.shape)\n",
    "label_size_df.rename(columns={'index':'old_index'}, inplace=True)\n",
    "print(label_size_df)\n",
    "\n",
    "# Validation and Test. Sample from rest of verified dataframe\n",
    "label_size_ALL_DATA = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/label_size_dataframe_with_deletions.csv\", index_col=0)\n",
    "old_shape = label_size_ALL_DATA.shape[0]\n",
    "label_size_ALL_DATA.drop(index=label_size_df['old_index'], inplace=True)\n",
    "# assert (old_shape - label_size_ALL_DATA.shape[0]) == label_size_df.shape[0], f\"{(old_shape - label_size_ALL_DATA.shape[0])} == {label_size_df.shape[0]}\"\n",
    "\n",
    "VAL_TEST_SIZE = 1500\n",
    "sampled_data = label_size_ALL_DATA.sample(VAL_TEST_SIZE, replace=False)['label_name']\n",
    "x_test, x_val, _, _ = train_test_split(sampled_data, sampled_data, test_size=0.5, random_state=1)\n",
    "\n",
    "# -------  Normal ------- \n",
    "# Create even training datasets for both. Populates train/labels and train/images\n",
    "title, img_location = ('normal', \"images_grayscale_16bit\")\n",
    "for img_name in label_size_df['label_name']:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\", f\"{project_output_dir_FINAL}/{title}/train/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}.png\",   f\"{project_output_dir_FINAL}/{title}/train/images/{img_name}.png\")\n",
    "for img_name in x_test:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\", f\"{project_output_dir_FINAL}/{title}/test/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}.png\",   f\"{project_output_dir_FINAL}/{title}/test/images/{img_name}.png\")\n",
    "for img_name in x_val:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\", f\"{project_output_dir_FINAL}/{title}/val/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}.png\",   f\"{project_output_dir_FINAL}/{title}/val/images/{img_name}.png\")\n",
    "\n",
    "# -------  Gradient ------- \n",
    "# Create even training datasets for both. Populates train/labels and train/images \n",
    "title, img_location = ('gradient', \"images_grayscale_16bit_gradient\")\n",
    "for img_name in label_size_df['label_name']:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\", f\"{project_output_dir_FINAL}/{title}/train/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}_gradient.png\",   f\"{project_output_dir_FINAL}/{title}/train/images/{img_name}.png\")    \n",
    "for img_name in x_test:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\",        f\"{project_output_dir_FINAL}/{title}/test/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}_gradient.png\", f\"{project_output_dir_FINAL}/{title}/test/images/{img_name}.png\")\n",
    "for img_name in x_val:\n",
    "    # Copy label\n",
    "    shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}.png\",        f\"{project_output_dir_FINAL}/{title}/val/labels/{img_name}.png\")\n",
    "    shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name}_gradient.png\", f\"{project_output_dir_FINAL}/{title}/val/images/{img_name}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
