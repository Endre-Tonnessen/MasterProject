{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter datasets to ensure they are inline with eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify images without a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_dir = \"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data\"\n",
    "\n",
    "# image_files =  [file[:-4] for file in os.listdir(f\"{project_dir}/images\")]\n",
    "# label_YOLOv8 = [file[:-4] for file in os.listdir(f\"{project_dir}/labels_YOLOv8\")]\n",
    "# label_image =  [file[:-4] for file in os.listdir(f\"{project_dir}/labels_as_images\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-05_14.18.49--NAs--T1354-GFP_Burst_Frame_0_Granule_10\n",
      "Images       596833\n",
      "Label_YOLOv8 596833\n",
      "Label_image  595553\n"
     ]
    }
   ],
   "source": [
    "# print(image_files[0])\n",
    "# print(\"Images      \", len(image_files))\n",
    "# print(\"Label_YOLOv8\", len(label_YOLOv8))\n",
    "# print(\"Label_image \", len(label_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for im in image_files: # For every image\n",
    "#     if (im in label_YOLOv8) and (im in label_image): # If image has both yolo label and image label\n",
    "#         # Put valid image and labels into new folders\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/images/{im}.png\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/images/{im}.png\")\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/labels_as_images/{im}.png\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/labels_as_images/{im}.png\")\n",
    "#         shutil.copy(f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/labels_YOLOv8/{im}.txt\", f\"D:/Master/MasterProject/dataset_creation/datasets/cutout_with_padding/all_data/FINAL_DATASET/labels_YOLOv8/{im}.txt\")\n",
    "#         count += 1\n",
    "# print(f\"Final dataset size is:\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train, val, test datasets of given size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset sizes ---\n",
      "3500\n",
      "3500\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "# compiled_datasets_name = \"compiled_datasets_16bit_large\"\n",
    "\n",
    "# project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "# project_output_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/{compiled_datasets_name}\"\n",
    "\n",
    "# # Create folder structure\n",
    "# Path(project_output_dir_FINAL).mkdir(parents=True, exist_ok=True)\n",
    "# for s in ['train', 'val', 'test']:\n",
    "#     Path(project_output_dir_FINAL+f\"/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "#     for st in ['images', 'labels']:\n",
    "#         Path(project_output_dir_FINAL+f\"/{s}/{st}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # image_files_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/images\")]\n",
    "# img_directory = \"images_grayscale_16bit\" # \"images\"\n",
    "# image_files_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/{img_directory}\")]\n",
    "# # label_YOLOv8_FINAL = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_YOLOv8\")]\n",
    "# label_image_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_as_images\")]\n",
    "\n",
    "# TOTAL_SIZE = 10000\n",
    "# subset_dataset = []\n",
    "# for i in np.random.choice(image_files_FINAL, TOTAL_SIZE, replace=False):\n",
    "#     subset_dataset.append(i)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(subset_dataset, subset_dataset, test_size=0.3, random_state=1)\n",
    "# x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "# print(\"--- Dataset sizes ---\")\n",
    "# print(len(x_train))\n",
    "# print(len(y_train))\n",
    "# print(len(x_val))\n",
    "# print(len(y_val))\n",
    "# print(len(x_test))\n",
    "# print(len(y_test))\n",
    "\n",
    "# for im in x_train:\n",
    "#     shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/train/images/{im}.png\")\n",
    "#     shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/train/labels/{im}.png\")\n",
    "\n",
    "# for im in x_val: \n",
    "#     shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/val/images/{im}.png\")\n",
    "#     shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/val/labels/{im}.png\")\n",
    "\n",
    "# for im in x_test: \n",
    "#     shutil.copy(f\"{project_dir_FINAL}/{img_directory}/{im}.png\",  f\"{project_output_dir_FINAL}/test/images/{im}.png\")\n",
    "#     shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{im}.png\", f\"{project_output_dir_FINAL}/test/labels/{im}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing dataset based on size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/DATASET_DIVIDED_ON_SIZE\"\n",
    "project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "\n",
    "# # Create folder structure\n",
    "# for s in [10,20,30,40,50,60,70,80,90]:\n",
    "#     Path(project_dir+f\"/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "label_size_dataframe = {\n",
    "    'label_name': [],\n",
    "    'label_size': [],\n",
    "}\n",
    "# Only need labels\n",
    "label_image_FINAL  = [file[:-4] for file in os.listdir(f\"{project_dir_FINAL}/labels_as_images\")]\n",
    "for i in range(len(label_image_FINAL)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Status:\", i)\n",
    "    feature_image = cv2.imread(f\"{project_dir_FINAL}/labels_as_images/{label_image_FINAL[i]}.png\", cv2.IMREAD_GRAYSCALE)\n",
    "    label_size_dataframe['label_name'].append(label_image_FINAL[i])\n",
    "    label_size_dataframe['label_size'].append(feature_image[feature_image == 1].size / (1024*1024))\n",
    "    assert feature_image[feature_image == 1].size == np.sum(feature_image)\n",
    "\n",
    "pd.DataFrame(label_size_dataframe).to_csv(project_dir + \"/label_size_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\\DATASET_DIVIDED_ON_SIZE\\label_size_dataframe.csv\", index_col=0)\n",
    "label_size_df = label_size_df[(0.1 < label_size_df['label_size']) & (label_size_df['label_size'] < 0.48)]\n",
    "pd.DataFrame(label_size_df).to_csv(project_dir + \"/label_size_dataframe_with_deletions.csv\")\n",
    "\n",
    "# Read all csv in \"D:\\Master\\MasterProject\\dataset_creation\\datasets\\FINAL_DATASET_cutout_with_padding\\images_saved_analysis_data\"\n",
    "# Apply size filer?\n",
    "# Run even dataset script\n",
    "# Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csvs() -> pd.DataFrame:\n",
    "    big_df: pd.DataFrame = None\n",
    "    project_dir = \"D:\\Master\\MasterProject\\dataset_creation\\datasets\\FINAL_DATASET_cutout_with_padding\\images_saved_analysis_data\"\n",
    "\n",
    "    # image_files_FINAL  = [file[12:-4] for file in os.listdir(f\"{training_results}\")]\n",
    "    csv_files = [file for file in os.listdir(f\"{project_dir}\") if os.path.isfile(f\"{project_dir}\\{file}\")]\n",
    "    for f in csv_files:\n",
    "        if f.split('__') == ['']:\n",
    "            continue\n",
    "        df = pd.read_csv(f\"{project_dir}\\{f}\", index_col=0)\n",
    "        # df['architecture'] = architecture\n",
    " \n",
    "        if type(big_df) == 'NoneType':\n",
    "            big_df = df\n",
    "        else:\n",
    "            big_df = pd.concat((big_df, df))\n",
    "    # --- Add expierment names ---\n",
    "    conditions = [\n",
    "        (big_df['filename'].str.contains('NCz')),\n",
    "        (big_df['filename'].str.contains('NControlLongB')), # Ctrlomozone \n",
    "        (big_df['filename'].str.contains('NAs')),\n",
    "        ]\n",
    "    values = ['NControlLongB', 'NCz', 'NAs']\n",
    "    big_df['experiment'] = np.select(conditions, values)\n",
    "\n",
    "    return big_df.reset_index(drop=True)\n",
    "\n",
    "post_creation_csv = read_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>frame</th>\n",
       "      <th>granule_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label_size</th>\n",
       "      <th>label_size_percent</th>\n",
       "      <th>filename_full</th>\n",
       "      <th>experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burst</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>221134.0</td>\n",
       "      <td>0.210890</td>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burs...</td>\n",
       "      <td>NCz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burst</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>229390.0</td>\n",
       "      <td>0.218763</td>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burs...</td>\n",
       "      <td>NCz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burst</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>208883.0</td>\n",
       "      <td>0.199206</td>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burs...</td>\n",
       "      <td>NCz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burst</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>207636.0</td>\n",
       "      <td>0.198017</td>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burs...</td>\n",
       "      <td>NCz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burst</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>213014.0</td>\n",
       "      <td>0.203146</td>\n",
       "      <td>2019-10-31_10.52.27--NControlLongB--T1015-Burs...</td>\n",
       "      <td>NCz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457877</th>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst</td>\n",
       "      <td>999</td>\n",
       "      <td>730</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>185369.0</td>\n",
       "      <td>0.176782</td>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...</td>\n",
       "      <td>NAs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457878</th>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst</td>\n",
       "      <td>999</td>\n",
       "      <td>225</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>227595.0</td>\n",
       "      <td>0.217052</td>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...</td>\n",
       "      <td>NAs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457879</th>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst</td>\n",
       "      <td>999</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>203373.0</td>\n",
       "      <td>0.193952</td>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...</td>\n",
       "      <td>NAs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457880</th>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst</td>\n",
       "      <td>999</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>182697.0</td>\n",
       "      <td>0.174233</td>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...</td>\n",
       "      <td>NAs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457881</th>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst</td>\n",
       "      <td>999</td>\n",
       "      <td>913</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>165170.0</td>\n",
       "      <td>0.157518</td>\n",
       "      <td>2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...</td>\n",
       "      <td>NAs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457882 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filename  frame  granule_id  \\\n",
       "0       2019-10-31_10.52.27--NControlLongB--T1015-Burst      0           1   \n",
       "1       2019-10-31_10.52.27--NControlLongB--T1015-Burst      0           3   \n",
       "2       2019-10-31_10.52.27--NControlLongB--T1015-Burst      0           5   \n",
       "3       2019-10-31_10.52.27--NControlLongB--T1015-Burst      0           6   \n",
       "4       2019-10-31_10.52.27--NControlLongB--T1015-Burst      0           7   \n",
       "...                                                 ...    ...         ...   \n",
       "457877        2020-02-05_14.43.03--NAs--T1354-GFP_Burst    999         730   \n",
       "457878        2020-02-05_14.43.03--NAs--T1354-GFP_Burst    999         225   \n",
       "457879        2020-02-05_14.43.03--NAs--T1354-GFP_Burst    999          56   \n",
       "457880        2020-02-05_14.43.03--NAs--T1354-GFP_Burst    999          60   \n",
       "457881        2020-02-05_14.43.03--NAs--T1354-GFP_Burst    999         913   \n",
       "\n",
       "        width  height  label_size  label_size_percent  \\\n",
       "0          27      23    221134.0            0.210890   \n",
       "1          25      24    229390.0            0.218763   \n",
       "2          24      23    208883.0            0.199206   \n",
       "3          22      22    207636.0            0.198017   \n",
       "4          23      22    213014.0            0.203146   \n",
       "...       ...     ...         ...                 ...   \n",
       "457877     20      23    185369.0            0.176782   \n",
       "457878     23      23    227595.0            0.217052   \n",
       "457879     22      23    203373.0            0.193952   \n",
       "457880     22      18    182697.0            0.174233   \n",
       "457881     20      20    165170.0            0.157518   \n",
       "\n",
       "                                            filename_full experiment  \n",
       "0       2019-10-31_10.52.27--NControlLongB--T1015-Burs...        NCz  \n",
       "1       2019-10-31_10.52.27--NControlLongB--T1015-Burs...        NCz  \n",
       "2       2019-10-31_10.52.27--NControlLongB--T1015-Burs...        NCz  \n",
       "3       2019-10-31_10.52.27--NControlLongB--T1015-Burs...        NCz  \n",
       "4       2019-10-31_10.52.27--NControlLongB--T1015-Burs...        NCz  \n",
       "...                                                   ...        ...  \n",
       "457877  2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...        NAs  \n",
       "457878  2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...        NAs  \n",
       "457879  2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...        NAs  \n",
       "457880  2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...        NAs  \n",
       "457881  2020-02-05_14.43.03--NAs--T1354-GFP_Burst_Fram...        NAs  \n",
       "\n",
       "[457882 rows x 9 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_creation_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(415870, 10) vs (453807, 10)\n"
     ]
    }
   ],
   "source": [
    "label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/FULL_DATASET.csv\")\n",
    "s = label_size_df[label_size_df['frame'] <= 1500].shape\n",
    "print(f\"{s} vs {label_size_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL_DATASET.csv already exists! Skipping. \n",
      "(19500, 10)\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "        \n",
    "my_file = Path(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/FULL_DATASET.csv\")\n",
    "if not my_file.is_file():\n",
    "    # label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/label_size_dataframe_with_deletions.csv\", index_col=0)\n",
    "    def read_csvs(filter_extreme_values=True) -> pd.DataFrame:\n",
    "        big_df: pd.DataFrame = None\n",
    "        project_dir = \"D:\\Master\\MasterProject\\dataset_creation\\datasets\\FINAL_DATASET_cutout_with_padding\\images_saved_analysis_data\"\n",
    "\n",
    "        # image_files_FINAL  = [file[12:-4] for file in os.listdir(f\"{training_results}\")]\n",
    "        csv_files = [file for file in os.listdir(f\"{project_dir}\") if os.path.isfile(f\"{project_dir}\\{file}\")]\n",
    "        for f in csv_files:\n",
    "            if f.split('__') == ['']:\n",
    "                continue\n",
    "            df = pd.read_csv(f\"{project_dir}\\{f}\", index_col=0)\n",
    "            # df['architecture'] = architecture\n",
    "    \n",
    "            if type(big_df) == 'NoneType':\n",
    "                big_df = df\n",
    "            else:\n",
    "                big_df = pd.concat((big_df, df))\n",
    "        # --- Add expierment names ---\n",
    "        conditions = [\n",
    "            (big_df['filename'].str.contains('NCz')),\n",
    "            (big_df['filename'].str.contains('NControlLongB')), # Ctrlomozone \n",
    "            (big_df['filename'].str.contains('NAs')),\n",
    "            ]\n",
    "        values = ['NControlLongB', 'NCz', 'NAs']\n",
    "        big_df['experiment'] = np.select(conditions, values)\n",
    "\n",
    "        if filter_extreme_values == True:\n",
    "            big_df = big_df[(0.1 < big_df['label_size_percent']) & (big_df['label_size_percent'] < 0.48)] # Remove extreme values\n",
    "\n",
    "        return big_df.reset_index(drop=True)\n",
    "\n",
    "    label_size_df = read_csvs(filter_extreme_values=True)\n",
    "    label_size_df.to_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/FULL_DATASET.csv\")\n",
    "else:\n",
    "    print(\"FULL_DATASET.csv already exists! Skipping. \")\n",
    "    label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/FULL_DATASET.csv\")\n",
    "    label_size_df = label_size_df[label_size_df['frame'] <= 1500]\n",
    "\n",
    "\n",
    "my_file = Path(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/EVEN_DATASET.csv\")\n",
    "if not my_file.is_file(): # \n",
    "    assert label_size_df[(0.1 > label_size_df['label_size_percent'])].empty, \"ERRORS IN DATASET\"\n",
    "    assert label_size_df[(0.5 < label_size_df['label_size_percent'])].empty, \"ERRORS IN DATASET\"\n",
    "\n",
    "    indexes = []\n",
    "    sample_size = 1500\n",
    "    space = np.linspace(0.1,0.55,20)\n",
    "    for i in range(len(space[:-1])): \n",
    "        # split: pd.DataFrame = label_size_df[(i < label_size_df['label_size_percent']) & (label_size_df['label_size_percent'] < (i+0.025))]\n",
    "        split: pd.DataFrame = label_size_df[(space[i] < label_size_df['label_size_percent']) & (label_size_df['label_size_percent'] <= (space[i+1]))]\n",
    "        if (split.size == 0) or (split.__len__() < sample_size):\n",
    "            continue\n",
    "        if verbose:\n",
    "            print(f\"({round(space[i],3)} < x < {round(space[i+1],3)})\",split.size)\n",
    "            print(\"-----------------\") # 244\n",
    "        # if split.size > 600:\n",
    "        sample_labels = list(split.sample(sample_size, replace=False).index.values)\n",
    "        indexes += (sample_labels)\n",
    "        # print(split.index)\n",
    "        \n",
    "    # get leftover\n",
    "    # split: pd.DataFrame = label_size_df[(0.1 < label_size_df['label_size']) & (label_size_df['label_size'] < (0.15))].index.values\n",
    "    # indexes += (sample_labels)\n",
    "\n",
    "    # print(split.__len__())\n",
    "    # even_dataset = label_size_df.loc[indexes].reset_index(drop=True)\n",
    "    even_dataset = label_size_df.loc[indexes]\n",
    "    even_dataset.to_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/EVEN_DATASET.csv\")\n",
    "    print(even_dataset.shape)\n",
    "    # even_dataset\n",
    "else:\n",
    "    print(\"EVEN_DATASET already exists! Skipping. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data based on label size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19500, 10)\n",
      "Index(['Unnamed: 0', 'filename', 'frame', 'granule_id', 'width', 'height',\n",
      "       'label_size', 'label_size_percent', 'filename_full', 'experiment'],\n",
      "      dtype='object')\n",
      "2019-12-09_14.13.37--NCz--T1334--GFP_Burst_Frame_216_Granule_60.png\n",
      "2019-12-09_14.13.37--NCz--T1334--GFP_Burst\n"
     ]
    }
   ],
   "source": [
    "label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/EVEN_DATASET.csv\", index_col=0) \n",
    "print(label_size_df.shape)\n",
    "print(label_size_df.columns)\n",
    "print(label_size_df['filename_full'].iloc[0])\n",
    "print(label_size_df['filename'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19500, 10)\n",
      "Index(['Unnamed: 0', 'filename', 'frame', 'granule_id', 'width', 'height',\n",
      "       'label_size', 'label_size_percent', 'filename_full', 'experiment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "compiled_datasets_name = \"compiled_datasets_16bit_20k_even_labels\"\n",
    "\n",
    "project_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding\"\n",
    "project_output_dir_FINAL = f\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/compiled/{compiled_datasets_name}\"\n",
    "\n",
    "my_file = Path(project_output_dir_FINAL)\n",
    "if not my_file.is_dir(): # \n",
    "    # Create folder structure\n",
    "    Path(project_output_dir_FINAL).mkdir(parents=True, exist_ok=True)\n",
    "    for title in ['normal', 'gradient']:\n",
    "        Path(project_output_dir_FINAL+f\"/{title}\").mkdir(parents=True, exist_ok=True) # Dataset folder\n",
    "\n",
    "        for s in ['train', 'val', 'test']:\n",
    "            Path(project_output_dir_FINAL+f\"/{title}/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "            for st in ['images', 'labels']:\n",
    "                Path(project_output_dir_FINAL+f\"/{title}/{s}/{st}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    label_size_df = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/EVEN_DATASET.csv\", index_col=0) \n",
    "    assert (label_size_df.shape == label_size_df[label_size_df['frame'] <= 1500].shape), \"There are more frames than 1500!\"\n",
    "    print(label_size_df.shape)\n",
    "    print(label_size_df.columns)\n",
    "\n",
    "    # Validation and Test. Sample from rest of verified dataframe\n",
    "    label_size_ALL_DATA = pd.read_csv(\"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/FULL_DATASET.csv\", index_col=0)\n",
    "    label_size_ALL_DATA.drop(label_size_df.index, inplace=True) # Remove all granules which are in training dataset. Prevents data cross contamination\n",
    "    label_size_ALL_DATA = label_size_ALL_DATA[label_size_ALL_DATA['frame'] <= 1500]\n",
    "    old_shape = label_size_ALL_DATA.shape[0]\n",
    "    # label_size_ALL_DATA.drop(index=label_size_df['old_index'], inplace=True)\n",
    "    # assert (old_shape - label_size_ALL_DATA.shape[0]) == label_size_df.shape[0], f\"{(old_shape - label_size_ALL_DATA.shape[0])} == {label_size_df.shape[0]}\"\n",
    "\n",
    "    VAL_TEST_SIZE = 6000\n",
    "    sampled_data = label_size_ALL_DATA.sample(VAL_TEST_SIZE, replace=False)['filename_full']\n",
    "    x_test, x_val, _, _ = train_test_split(sampled_data, sampled_data, test_size=0.5, random_state=1)\n",
    "\n",
    "    # -------  Normal ------- \n",
    "    # Create even training datasets for both. Populates train/labels and train/images\n",
    "    title, img_location = ('normal', \"images_grayscale_16bit\")\n",
    "    for img_name in label_size_df['filename_full']:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/train/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}.png\",   f\"{project_output_dir_FINAL}/{title}/train/images/{img_name[:-4]}.png\")\n",
    "    for img_name in x_test:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/test/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}.png\",   f\"{project_output_dir_FINAL}/{title}/test/images/{img_name[:-4]}.png\")\n",
    "    for img_name in x_val:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/val/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}.png\",   f\"{project_output_dir_FINAL}/{title}/val/images/{img_name[:-4]}.png\")\n",
    "\n",
    "    # -------  Gradient ------- \n",
    "    # Create even training datasets for both. Populates train/labels and train/images \n",
    "    title, img_location = ('gradient', \"images_grayscale_16bit_gradient\")\n",
    "    for img_name in label_size_df['filename_full']:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/train/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}_gradient.png\", f\"{project_output_dir_FINAL}/{title}/train/images/{img_name[:-4]}.png\")    \n",
    "    for img_name in x_test:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/test/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}_gradient.png\", f\"{project_output_dir_FINAL}/{title}/test/images/{img_name[:-4]}.png\")\n",
    "    for img_name in x_val:\n",
    "        # Copy label\n",
    "        shutil.copy(f\"{project_dir_FINAL}/labels_as_images/{img_name}\", f\"{project_output_dir_FINAL}/{title}/val/labels/{img_name}\")\n",
    "        shutil.copy(f\"{project_dir_FINAL}/{img_location}/{img_name[:-4]}_gradient.png\", f\"{project_output_dir_FINAL}/{title}/val/images/{img_name[:-4]}.png\")\n",
    "\n",
    "else:\n",
    "    print(f\"{compiled_datasets_name} is already compiled!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
