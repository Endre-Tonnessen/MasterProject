{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/compiled_datasets_medium\"\n",
    "DATA_DIR = \"D:/Master/MasterProject/dataset_creation/datasets/FINAL_DATASET_cutout_with_padding/compiled_datasets_16bit_tiny\"\n",
    "# compiled_datasets_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train/images')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train/labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val/images')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val/labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test/images')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Writing helper class for data extraction, tranformation and preprocessing  \n",
    "https://pytorch.org/docs/stable/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from dataset import Dataset as Dataset2, get_preprocessing\n",
    "import albumentations as albu\n",
    "CHANNELS_IN_IMAGE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dataset2(BaseDataset):    \n",
    "#     CLASSES = ['granule']\n",
    "    \n",
    "#     def __init__(\n",
    "#             self, \n",
    "#             images_dir, \n",
    "#             masks_dir, \n",
    "#             classes=None, \n",
    "#             augmentation=None, \n",
    "#             preprocessing=None,\n",
    "#     ):\n",
    "#         self.ids = os.listdir(images_dir)\n",
    "#         self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "#         self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "#         # convert str names to class values on masks\n",
    "#         self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "#         self.augmentation = augmentation\n",
    "#         self.preprocessing = preprocessing\n",
    "    \n",
    "#     def __getitem__(self, i):\n",
    "        \n",
    "#         # read data\n",
    "#         image = cv2.imread(self.images_fps[i], cv2.IMREAD_UNCHANGED)\n",
    "#         assert image.shape == (1024,1024)\n",
    "#         assert image[524,524] > 260, \"Ensure 16bit\"\n",
    "        \n",
    "#         # image = cv2.imread(self.images_fps[i])\n",
    "#         # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # <- Read as 1 channel!\n",
    "#         # print(image.shape)\n",
    "#         # print(image[524,524])\n",
    "#         mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "#         # extract certain classes from mask (e.g. cars)\n",
    "#         # masks = [(mask == 215) for v in self.class_values]\n",
    "#         # mask = np.stack(masks, axis=-1).astype('float')\n",
    "#         # Convert to binary mask\n",
    "#         mask[mask == 30]  = 0\n",
    "#         mask[mask == 215] = 1\n",
    "#         mask = np.stack([mask], axis=-1).astype('float')\n",
    "#         # print(\"Mask shape\")\n",
    "#         # print(mask.shape)\n",
    "        \n",
    "#         # apply augmentations\n",
    "#         if self.augmentation:\n",
    "#             sample = self.augmentation(image=image, mask=mask)\n",
    "#             image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "#         # apply preprocessing\n",
    "#         if self.preprocessing:\n",
    "#             sample = self.preprocessing(image=image, mask=mask)\n",
    "#             image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "#         return image, mask\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset2(x_train_dir, y_train_dir, classes=['granule'], CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE)\n",
    "\n",
    "image, mask = dataset[23] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    granule_mask=mask.squeeze(-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_tensor(x, **kwargs):\n",
    "#     # print(\"dasdasda\")\n",
    "#     # print(x.shape)\n",
    "#     if x.shape == (1024,1024):\n",
    "#         # print(\"---\")\n",
    "#         # print(x.shape)\n",
    "#         # x = torch.from_numpy(x.astype('float32')).unsqueeze(0).permute(0,1,2)\n",
    "#         x = np.expand_dims(x, 0).transpose(0,1,2).astype('float32')\n",
    "#         # print(x.shape)\n",
    "#         # print(\"---\")\n",
    "#         return x\n",
    "#     return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "# def get_preprocessing(preprocessing_fn):\n",
    "#     \"\"\"Construct preprocessing transform\n",
    "    \n",
    "#     Args:\n",
    "#         preprocessing_fn (callbale): data normalization function \n",
    "#             (can be specific for each pretrained neural network)\n",
    "#     Return:\n",
    "#         transform: albumentations.Compose\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     _transform = [\n",
    "#         # albu.Lambda(image=preprocessing_fn),\n",
    "#         albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "#     ]\n",
    "#     return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize resulted augmented images and masks\n",
    "augmented_dataset = Dataset2(\n",
    "    x_train_dir, \n",
    "    y_train_dir,  \n",
    "    # preprocessing=get_preprocessing(None),\n",
    "    classes=['granule'],\n",
    "    CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE\n",
    ")\n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(1):\n",
    "    image, mask = augmented_dataset[i]\n",
    "    print(\"---\")\n",
    "    print(image.shape)\n",
    "    print(mask.shape)\n",
    "    visualize(image=image, mask=mask.squeeze())\n",
    "    # visualize(image=image, mask=mask.squeeze(-1))\n",
    "    #torch.Size([1, 1024, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.utils.metrics import IoU, Accuracy, Fscore\n",
    "from segmentation_models_pytorch.utils.train import TrainEpoch, ValidEpoch\n",
    "from segmentation_models_pytorch.utils.losses import DiceLoss, BCELoss, MSELoss, JaccardLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet34'#'efficientnet-b0' #'efficientnet-b1'#'resnet101' #'mit_b1' #efficientnet-b0' #'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['granule']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "# model = smp.FPN(\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    in_channels=1\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in model.encoder.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "list(model.encoder.children())\n",
    "# list(model.parameters())\n",
    "decoder_total_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "encoder_total_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "seg_head_total_params = sum(p.numel() for p in model.segmentation_head.parameters())\n",
    "print(f\"{decoder_total_params} + {encoder_total_params} + {seg_head_total_params} = {decoder_total_params+encoder_total_params+seg_head_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_new_image = cv2.imread(\"D:\\Master\\MasterProject\\dataset_creation\\datasets\\cutout_with_padding\\\\all_data\\images\\\\2020-02-05_14.18.49--NAs--T1354-GFP_Burst_Frame_0_Granule_3.png\", cv2.IMREAD_COLOR)\n",
    "# # cv2.imshow('asd', my_new_image)\n",
    "# # cv2.waitKey(0)\n",
    "# my_new_image.shape\n",
    "# my_new_image = torch.from_numpy(my_new_image)\n",
    "# my_new_image = my_new_image.permute(2, 0, 1) # H W N -> N H W\n",
    "# my_new_image = my_new_image / 255\n",
    "# my_new_image = torch.unsqueeze(my_new_image, 0)\n",
    "\n",
    "# print(my_new_image.shape)\n",
    "# print(my_new_image[0, 2, 3])\n",
    "\n",
    "# out = model(my_new_image)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset2(\n",
    "    x_train_dir, #x_test_dir,#x_train_dir, \n",
    "    y_train_dir, #y_test_dir,#, \n",
    "    # augmentation=get_training_augmentation(), \n",
    "    # preprocessing=get_preprocessing(None),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    "    CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset2(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    # augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    # preprocessing=get_preprocessing(None),\n",
    "    classes=CLASSES,\n",
    "    CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "# for i in range(3):\n",
    "#     image, mask = train_dataset[i]\n",
    "#     # print(image.shape)\n",
    "#     visualize(image=image, mask=mask.squeeze(-1))\n",
    "    # image = image.transpose(1, 2, 0).astype('float32')\n",
    "    # visualize(image=image, mask=mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "# loss = BCELoss() #DiceLoss() #smp.losses.DiceLoss(mode='binary')#utils.losses.DiceLoss()\n",
    "loss = JaccardLoss() #DiceLoss() #smp.losses.DiceLoss(mode='binary')#utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    IoU(threshold=0.5), #utils.metrics.IoU(threshold=0.5),\n",
    "    Accuracy(),\n",
    "    Fscore()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({metric_name._get_name():[] for metric_name in metrics})\n",
    "df[loss._get_name()] = []\n",
    "df\n",
    "\n",
    "df_train = {\"train_\"+metric_name._get_name():[] for metric_name in metrics}\n",
    "df_train[\"train_\"+loss._get_name()] = []\n",
    "df_val = {\"val_\"+metric_name._get_name():[] for metric_name in metrics}\n",
    "df_val[\"val_\"+loss._get_name()] = []\n",
    "df_train.update(df_val)\n",
    "df = df_train\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training(df:dict, train_logs: dict, validation_logs:dict):\n",
    "    train_logs = {\"train_\"+k:v for k,v in train_logs.items()}\n",
    "    validation_logs = {\"val_\"+k:v for k,v in validation_logs.items()}\n",
    "    \n",
    "    for k,v in train_logs.items():\n",
    "        df[k].append(v)\n",
    "    for k,v in validation_logs.items():\n",
    "        df[k].append(v)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_dict(metrics, loss):\n",
    "    df_train = {\"train_\"+metric_name.__name__:[] for metric_name in metrics}\n",
    "    df_train[\"train_\"+loss.__name__] = []\n",
    "    df_val = {\"val_\"+metric_name.__name__:[] for metric_name in metrics}\n",
    "    df_val[\"val_\"+loss.__name__] = []\n",
    "    df_train.update(df_val)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "# Should be \n",
    "# torch.Size([4, 1, 1024, 1024])\n",
    "# torch.Size([4, 1, 1024, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "# Create logging dict\n",
    "\n",
    "df = create_log_dict(metrics, loss)\n",
    "print(df)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    df = log_training(df, train_logs, valid_logs) # Save stats\n",
    "\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 7:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n",
    "# df = pd.DataFrame(df)\n",
    "# df.to_csv('te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated() # 12671343104 - 9704958464\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "# best_model = torch.load('MODELS/best_model__DeepLabV3Plus__resnet101__DiceLoss__Freeze_encoder_False.pth')\n",
    "best_model = torch.load('./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = Dataset2(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    # augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    "    CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model on test set\n",
    "# test_epoch = smp.utils.train.ValidEpoch(\n",
    "#     model=best_model,\n",
    "#     loss=loss,\n",
    "#     metrics=metrics,\n",
    "#     device=DEVICE,\n",
    "# )\n",
    "\n",
    "# logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = Dataset2(\n",
    "    x_test_dir, y_test_dir, \n",
    "    classes=CLASSES,\n",
    "    CHANNELS_IN_IMAGE=CHANNELS_IN_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0]\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0]\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop wrapped with profiler object\n",
    "# with torch.profiler.profile(\n",
    "#         schedule=torch.profiler.schedule(wait=1, warmup=4, active=3, repeat=1),\n",
    "#         on_trace_ready=torch.profiler.tensorboard_trace_handler('./example'),\n",
    "#         record_shapes=True,\n",
    "#         profile_memory=True,\n",
    "#         with_stack=True\n",
    "# ) as prof:\n",
    "#     for step, data in enumerate(train_loader):\n",
    "#         inputs = data[0].to(device=device, non_blocking=True)\n",
    "#         labels = data[1].to(device=device, non_blocking=True)\n",
    "#         inputs = (inputs.to(torch.float32) / 255. - 0.5) / 0.5\n",
    "#         if step >= (1 + 4 + 3) * 1:\n",
    "#             break\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         prof.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
